import streamlit as st
import openai
from PyPDF2 import PdfReader
import io
import csv
import pandas as pd
from tqdm import tqdm
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI

openai_api_key = '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'
openai.api_key = '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'

# è®¾ç½®é¡µé¢å®½åº¦ä¸ºè¾ƒå®½çš„å¸ƒå±€
st.set_page_config(layout="wide")

def analyze_resume(jd, resume, options):
    df = analyze_str(resume, options)
    #print(df)
    df_string = df.applymap(lambda x: ', '.join(x) if isinstance(x, list) else x).to_string(index=False)
    #print(df_string)
    st.write("OpenAIç»¼åˆåˆ†æ..")
    summary_question = f"èŒä½è¦æ±‚æ˜¯ï¼š{{{jd}}}" + f"ç®€å†æ¦‚è¦æ˜¯ï¼š{{{df_string}}}" + "ï¼Œè¯·ç›´æ¥è¿”å›è¯¥åº”è˜å²—ä½å€™é€‰äººåŒ¹é…åº¦æ¦‚è¦ï¼ˆæ§åˆ¶åœ¨200å­—ä»¥å†…ï¼‰;'"
    summary = ask_openAI(summary_question)
    print(summary)

    df.loc[len(df)] = ['ç»¼åˆæ¦‚è¦', summary]
    extra_info = "æ‰“åˆ†è¦æ±‚ï¼šå›½å†…top10å¤§å­¦+3åˆ†ï¼Œ985å¤§å­¦+2åˆ†ï¼Œ211å¤§å­¦+1åˆ†ï¼Œå¤´éƒ¨ä¼ä¸šç»å†+2åˆ†ï¼ŒçŸ¥åä¼ä¸š+1åˆ†ï¼Œæµ·å¤–èƒŒæ™¯+3åˆ†ï¼Œå¤–ä¼èƒŒæ™¯+1åˆ†ã€‚ "
    score_question = f"èŒä½è¦æ±‚æ˜¯ï¼š{{{jd}}}" + f"ç®€å†æ¦‚è¦æ˜¯ï¼š{{{df.to_string(index=False)}}}" + "ï¼Œè¯·ç›´æ¥è¿”å›è¯¥åº”è˜å²—ä½å€™é€‰äººçš„åŒ¹é…åˆ†æ•°ï¼ˆ0-100ï¼‰ï¼Œè¯·ç²¾ç¡®æ‰“åˆ†ä»¥æ–¹ä¾¿å…¶ä»–å€™é€‰äººå¯¹æ¯”æ’åºï¼Œ'" + extra_info

    score = ask_openAI(score_question)
    df.loc[len(df)] = ['åŒ¹é…å¾—åˆ†', score]

    return df

def ask_openAI(question):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=question,
        max_tokens=800,
        n=1,
        stop=None,
        temperature=0,
    )
    return response.choices[0].text.strip()

def analyze_str(resume, options):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=300,
        chunk_overlap=50,
        length_function=len
    )
    chunks = text_splitter.split_text(resume)

    # Open (or create) a csv file in write mode
    with open('chunks.csv', 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)

        # Write the header
        writer.writerow(["Index", "Chunk"])

        # Write the chunks with their respective indices
        for i, chunk in enumerate(chunks):
            writer.writerow([i, chunk])

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    knowledge_base = FAISS.from_texts(chunks, embeddings)

    df_data = [{'option': option, 'value': []} for option in options]
    st.write("ä¿¡æ¯æŠ“å–")

    # åˆ›å»ºè¿›åº¦æ¡å’Œç©ºå…ƒç´ 
    progress_bar = st.progress(0)
    option_status = st.empty()

    for i, option in tqdm(enumerate(options), desc="ä¿¡æ¯æŠ“å–ä¸­", unit="é€‰é¡¹", ncols=100):
        question = f"è¿™ä¸ªåº”è˜è€…çš„{option}æ˜¯ä»€ä¹ˆï¼Œè¯·ç²¾ç®€è¿”å›ç­”æ¡ˆï¼Œæœ€å¤šä¸è¶…è¿‡300å­—ï¼Œå¦‚æœæŸ¥æ‰¾ä¸åˆ°ï¼Œåˆ™è¿”å›'æœªæä¾›'"
        docs = knowledge_base.similarity_search(question)
        print(len(docs))
        llm = OpenAI(openai_api_key=openai_api_key,temperature=0.1, verbose=False)
        chain = load_qa_chain(llm, chain_type="stuff")
        response = chain.run(input_documents=docs, question=question )
        df_data[i]['value'] = response
        option_status.text(f"æ­£åœ¨æŸ¥æ‰¾ä¿¡æ¯ï¼š{option}")

        # æ›´æ–°è¿›åº¦æ¡
        progress = (i + 1) / len(options)
        progress_bar.progress(progress)

    df = pd.DataFrame(df_data)
    st.success("ç®€å†è¦ç´ å·²è·å–")
    return df



def main():

    # è®¾ç½®é¡µé¢æ ‡é¢˜
    st.title("ğŸš€ GPTæ‹›è˜åˆ†ææœºå™¨äºº")
    st.subheader("ğŸª¢ Langchain + ğŸ OpenAI")

    # è®¾ç½®é»˜è®¤çš„JDå’Œç®€å†ä¿¡æ¯
    default_jd = """é«˜çº§ç­–ç•¥äº§å“ç»ç† 3.5ä¸‡-4.5ä¸‡  15è–ª 
        èŒä½æè¿°:
        1ã€è´Ÿè´£Cç«¯ç­–ç•¥ç›¸å…³äº§å“çš„è®¾è®¡å·¥ä½œï¼Œè®¾è®¡é’ˆå¯¹ä¸åŒç”¨æˆ·å±‚æ¬¡çš„äº§å“æ‰¿æ¥ç­–ç•¥ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæœç´¢ã€æ¨èã€å³æ—¶æ¶ˆæ¯ã€æ¨é€ç­‰åœºæ™¯ï¼Œå¹¶è´Ÿè´£ç”¨æˆ·ç•™å­˜å’ŒæŠ•é€’é‡ã€‚
        2ã€æ·±å…¥æ´å¯Ÿæ‹›è˜è¡Œä¸šç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚åœºæ™¯ï¼Œä¸ç”¨æˆ·ç ”ç©¶å›¢é˜Ÿåˆä½œï¼Œæ¢ç´¢å¹¶ç†è§£ç”¨æˆ·çš„è¡Œä¸ºã€åå¥½å’Œç—›ç‚¹ï¼Œå°†è¿™äº›æ´å¯Ÿè½¬åŒ–ä¸ºå‰ç«¯äº§å“å½¢æ€çš„è®¾è®¡æ–¹å‘ã€‚
        3ã€ä¸ç®—æ³•ç­–ç•¥å›¢é˜Ÿç´§å¯†åä½œï¼Œç†è§£æ¨èç³»ç»Ÿçš„åŸºæœ¬æ¡†æ¶å’Œæœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†ï¼Œæä¾›äº§å“éœ€æ±‚å’ŒæŒ‡å¯¼ï¼Œç¡®ä¿äº§å“çš„æ¨èç­–ç•¥ä¸ç”¨æˆ·ä½“éªŒå’Œæ•°æ®æŒ‡æ ‡çš„æ­£å‘å‘å±•ä¸€è‡´ã€‚
        4ã€ä¸å…¶ä»–ä¸šåŠ¡å›¢é˜Ÿç´§å¯†åˆä½œï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè¿è¥ã€æŠ€æœ¯å’Œè®¾è®¡å›¢é˜Ÿï¼Œå…±åŒæå‡ç”¨æˆ·ä½“éªŒå’Œæµé‡æ•ˆç‡ï¼Œæ¨åŠ¨äº§å“çš„æŒç»­ä¼˜åŒ–å’Œåˆ›æ–°ã€‚
        5ã€é€šè¿‡æ•°æ®å®šé‡åˆ†æã€å®šæ€§è§‚å¯Ÿå’Œç”¨æˆ·åé¦ˆï¼Œæ·±å…¥äº†è§£ç”¨æˆ·è¡Œä¸ºå’Œäº§å“æ•ˆæœï¼Œä¸æ–­æ”¹è¿›äº§å“å’Œç­–ç•¥æ–¹æ¡ˆï¼Œæé«˜ç”¨æˆ·æ»¡æ„åº¦å’Œä¸šåŠ¡æˆæœã€‚
        èŒä½è¦æ±‚ï¼š\
        1ã€æœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼Œå…·å¤‡5-10å¹´æ¨èç­–ç•¥äº§å“ç»éªŒï¼Œæœ‰æ‹›è˜è¡Œä¸šç»éªŒè€…ä¼˜å…ˆè€ƒè™‘ã€‚
        2ã€å¯¹æ¨èç³»ç»Ÿçš„åŸºæœ¬æ¡†æ¶å’Œæœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†æœ‰æ·±å…¥çš„ç†è§£ï¼Œå¹¶èƒ½å°†å…¶åº”ç”¨äºäº§å“è®¾è®¡å’Œç­–ç•¥åˆ¶å®šä¸­ã€‚
        3ã€å…·å¤‡è‰¯å¥½çš„ç”¨æˆ·ä½“éªŒæ„ŸçŸ¥ï¼Œäº†è§£æ‹›è˜åœºæ™¯ä¸‹ç”¨æˆ·çš„éœ€æ±‚å’Œä½“éªŒè®¤çŸ¥ï¼Œèƒ½å¤Ÿå°†ç”¨æˆ·æ´å¯Ÿè½¬åŒ–ä¸ºå®é™…çš„äº§å“æ”¹è¿›å’Œåˆ›æ–°ã€‚
        4ã€æ•°æ®é©±åŠ¨æ€ç»´ï¼Œèƒ½å¤Ÿé€šè¿‡æ•°æ®åˆ†æå’Œç”¨æˆ·ç ”ç©¶æ¥æ”¯æŒäº§å“å†³ç­–ï¼Œå…·å¤‡å®šé‡å’Œå®šæ€§åˆ†æçš„èƒ½åŠ›ã€‚
        5ã€å…·å¤‡å‡ºè‰²çš„é¡¹ç›®åä½œèƒ½åŠ›å’Œæ²Ÿé€šèƒ½åŠ›ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ¨è¿›é¡¹ç›®ï¼Œè·¨å›¢é˜Ÿåˆä½œå¹¶ä¸å„æ–¹åˆ©ç›Šç›¸å…³è€…ä¿æŒè‰¯å¥½çš„æ²Ÿé€šã€‚
        6ã€å…·å¤‡å¼ºå¤§çš„æŠ—å‹èƒ½åŠ›å’Œé—®é¢˜è§£å†³èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¿«èŠ‚å¥çš„å·¥ä½œç¯å¢ƒä¸­å¤„ç†å¤šä»»åŠ¡å¹¶ä¿æŒé«˜è´¨é‡çš„å·¥ä½œæˆæœã€‚
        æˆ‘ä»¬å¸Œæœ›ä½ èƒ½ä¸ºæˆ‘ä»¬çš„å›¢é˜Ÿå¸¦æ¥åˆ›æ–°æ€ç»´å’Œç§¯æè¿›å–çš„æ€åº¦ï¼Œå¹¶ä¸æˆ‘ä»¬å…±åŒåŠªåŠ›æä¾›å“è¶Šçš„ç”¨æˆ·ä½“éªŒå’Œä¸šåŠ¡æˆæœã€‚
        å¦‚æœæ‚¨å¯¹è¯¥èŒä½æ„Ÿå…´è¶£ï¼Œè¯·æäº¤æ‚¨çš„ç®€å†åŠç›¸å…³ä½œå“ï¼Œæˆ‘ä»¬æœŸå¾…ä¸æ‚¨è¿›ä¸€æ­¥äº¤æµã€‚"""

    # è¾“å…¥JDä¿¡æ¯
    jd_text = st.text_area("ã€å²—ä½ä¿¡æ¯ã€‘", height=300, value=default_jd)

    # è¾“å…¥ç®€å†ä¿¡æ¯
    #resume_text = st.text_area("ã€åº”è˜ç®€å†ã€‘", height=100, value=default_resume)
    uploaded_file = st.file_uploader("ã€åº”è˜ç®€å†ã€‘", type="pdf")
    if uploaded_file is not None:
        pdf = PdfReader(io.BytesIO(uploaded_file.getvalue()))  
        resume_text = ""
        for page in range(len(pdf.pages)):  
            resume_text += pdf.pages[page].extract_text()  
        #st.text(resume_text)
        st.markdown(
            f'<div style="height:300px;overflow:auto;">{resume_text}</div>', 
            unsafe_allow_html=True
        )


    # å‚æ•°è¾“å…¥
    options = ["å§“å", "è”ç³»å·ç ", "æ€§åˆ«", "å¹´é¾„", "å·¥ä½œå¹´æ•°ï¼ˆæ•°å­—ï¼‰", "æœ€é«˜å­¦å†", "æœ¬ç§‘å­¦æ ¡åç§°", "ç¡•å£«å­¦æ ¡åç§°", "æ˜¯å¦åœ¨èŒ", "å½“å‰èŒåŠ¡", "å†å²ä»»èŒå…¬å¸åˆ—è¡¨", "æŠ€æœ¯èƒ½åŠ›", "ç»éªŒç¨‹åº¦", "ç®¡ç†èƒ½åŠ›"]
    selected_options = st.multiselect("è¯·é€‰æ‹©é€‰é¡¹", options, default=options)

    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆ†æ"):
        df = analyze_resume(jd_text, resume_text, selected_options)
        st.subheader("ç»¼åˆåŒ¹é…å¾—åˆ†ï¼š"+ df.loc[df['option'] == 'åŒ¹é…å¾—åˆ†', 'value'].values[0])
        st.subheader("ç»†é¡¹å±•ç¤ºï¼š")
        st.table(df)

if __name__ == "__main__":
    main()
